{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING : MAKE SURE YOU GO THROUGH pytorch_env_install.ipynb IN ORDER TO INSTALL PIPENV AND DEPENDENCIES\n",
    "Project done on VisualStudio Code, will automatically prompt for necessary extensions\n",
    "YOU NEED TO INSTALL PYTHON 3.7.9 ON YOUR SYSTEM !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from librarybuilding_utils import tipettNoiser_fromARRAY\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "import random\n",
    "import csv\n",
    "\n",
    "\"\"\"\n",
    "Defining file variables\n",
    "\"\"\"\n",
    "main_dir = \"./data2\"\n",
    "original_images_folder = \"/og_data\"\n",
    "train_directory_clean_clones = \"/train_set/train_noiseless_clones\"\n",
    "train_directory_noisy_clones = \"/train_set/train_noisy_clones\"\n",
    "test_directory_clean_clones = \"/test_set/test_noiseless_clones\"\n",
    "test_directory_noisy_clones = \"/test_set/test_noisy_clones\"\n",
    "\n",
    "def createIfNotExisting():\n",
    "    if not (os.path.exists(main_dir+train_directory_clean_clones)):\n",
    "        os.makedirs(main_dir+train_directory_clean_clones)\n",
    "        print(f\"created new directory {main_dir+train_directory_clean_clones}\")\n",
    "    if not (os.path.exists(main_dir+train_directory_noisy_clones)):\n",
    "        os.makedirs(main_dir+train_directory_noisy_clones)\n",
    "        print(f\"created new directory {main_dir+train_directory_noisy_clones}\")\n",
    "    if not (os.path.exists(main_dir+test_directory_clean_clones)):\n",
    "        os.makedirs(main_dir+test_directory_clean_clones)\n",
    "        print(f\"created new directory {main_dir+test_directory_clean_clones}\")\n",
    "    if not (os.path.exists(main_dir+test_directory_noisy_clones)):\n",
    "        os.makedirs(main_dir+test_directory_noisy_clones)\n",
    "        print(f\"created new directory {main_dir+test_directory_noisy_clones}\")\n",
    "\n",
    "def array_divider(img,sqr_size):\n",
    "    height,width = img.shape\n",
    "\n",
    "    print(f\"height = {height}, width = {width}\")\n",
    "\n",
    "    hrz = 0\n",
    "    vrt = 0\n",
    "    slices = []\n",
    "\n",
    "    while(vrt+sqr_size<width):\n",
    "        while(hrz+sqr_size<height):\n",
    "            newslice = img[vrt:vrt+sqr_size,hrz:hrz+sqr_size]\n",
    "            #If condition so that we dont have non square slices\n",
    "            if(newslice.shape[0] == newslice.shape[1]):\n",
    "                slices.append(img[vrt:vrt+sqr_size,hrz:hrz+sqr_size])   \n",
    "\n",
    "            hrz+=sqr_size\n",
    "        hrz=0\n",
    "        vrt+=sqr_size\n",
    "    \n",
    "    return slices\n",
    "\n",
    "def multi_flipper(array):\n",
    "    return np.array([np.flip(array, 0), np.flip(array, 1),np.flip(np.flip(array, 0),1)])\n",
    "\n",
    "def multi_rotate(array):\n",
    "    return np.array([np.rot90(array,1,(0,1)),np.rot90(array,2,(0,1)),np.rot90(array,3,(0,1))])\n",
    "\n",
    "def flipper(array,index):\n",
    "    if(index==2):\n",
    "        return np.flip(np.flip(array, 0),1)\n",
    "    return np.flip(array, index)\n",
    "\n",
    "def rotate(array, index):\n",
    "    return np.rot90(array,index,(0,1))\n",
    "\n",
    "#VERSION TROP GOUrMANDE EN MEMOIRE : ARRAY CONTENANT LES CLONES TROP VOLUMINEUX\n",
    "def getClones(img,sqr_size):\n",
    "    slices = array_divider(img,sqr_size)\n",
    "    clones = slices\n",
    "    for slice in slices:\n",
    "        tmp_flipped_slices = multi_flipper(slice)\n",
    "        for flipped_slice in tmp_flipped_slices:\n",
    "            clones.append(flipped_slice)\n",
    "    \n",
    "    clones_rot = clones\n",
    "    for clone in clones:\n",
    "        tmp_rot_slices = multi_rotate(clone)\n",
    "        for rot_slice in tmp_rot_slices:\n",
    "            clones_rot.append(rot_slice)\n",
    "\n",
    "    clones_noisy = np.array([])\n",
    "    for clone in clones_rot :\n",
    "        np.append(clones_noisy,tipettNoiser_fromARRAY(clone,3,1.3))\n",
    "\n",
    "    return clones,clones_noisy\n",
    "\n",
    "#UTILISATION DE YIELD POUR ECRIRE LES IMAGES UNE PAR UNE SUR LE DISQUE\n",
    "def getClones_yield(img,sqr_size):        \n",
    "    slices = array_divider(img,sqr_size)\n",
    "    for slice in slices:\n",
    "        for rot_index in [1,2,3]:\n",
    "            for flip_index in [0,1,2]:\n",
    "\n",
    "                clean_clones = rotate(flipper(slice,flip_index),rot_index)\n",
    "\n",
    "                noisy_clones = tipettNoiser_fromARRAY(clean_clones,1,1.3)\n",
    "                yield clean_clones,noisy_clones\n",
    "\n",
    "\n",
    "def createData_yield():\n",
    "\n",
    "    directory = os.fsencode(main_dir+original_images_folder)\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        print(filename)\n",
    "\n",
    "        og_img=np.load(os.fsdecode(directory)+\"/\"+filename)\n",
    "\n",
    "    \n",
    "        clones = getClones_yield(og_img,256)\n",
    "        index = 0\n",
    "        for clone in clones:\n",
    "            index+=1\n",
    "\n",
    "            #first member of the tuple is the rotate+flipper noise-FREE slice\n",
    "            PILimg = Image.fromarray(clone[0])\n",
    "            PILimg.convert(\"L\").save(main_dir+train_directory_clean_clones+\"/noiseless_\"+filename[:len(filename) -4]+\"_\"+str(index)+\".png\")\n",
    "\n",
    "            #first member of the tuple is the rotate+flipper noisY slice\n",
    "            PILimg = Image.fromarray(clone[1])\n",
    "            PILimg.convert(\"L\").save(main_dir+train_directory_noisy_clones+\"/noisy_\"+filename[:len(filename) -4]+\"_\"+str(index)+\".png\")\n",
    "\n",
    "\"\"\"\n",
    "This is a one shot execution script moving a fraction of the training database into the test\n",
    "database, following the \"test_ratio\" proportion\n",
    "Execute after running createData_yield()\n",
    "\"\"\"\n",
    "test_ratio = 0.1\n",
    "\n",
    "def sort_data_set():\n",
    "    to_be_moved = []\n",
    "    \n",
    "    for file in os.listdir(main_dir+train_directory_clean_clones):\n",
    "        filename = os.fsdecode(file)\n",
    "        if(random.random()<test_ratio):\n",
    "            #print(re.sub(\"noiseless_\", \"\", filename, count=0, flags=0))\n",
    "            to_be_moved.append(re.sub(\"noiseless_\", \"\", filename, count=0, flags=0))\n",
    "\n",
    "    for filename in to_be_moved:\n",
    "        os.replace(main_dir+train_directory_clean_clones+\"/noiseless_\"+filename, main_dir+test_directory_clean_clones+\"/noiseless_\"+filename)\n",
    "        os.replace(main_dir+train_directory_noisy_clones+\"/noisy_\"+filename, main_dir+test_directory_noisy_clones+\"/noisy_\"+filename)\n",
    "\n",
    "\"\"\"\n",
    "Creates a csv file that maps the original SAR images to their noisy clone\n",
    "\"\"\"\n",
    "\n",
    "def create_dataset_csv():\n",
    "    name_couples_test = {}    \n",
    "    for file in os.listdir(main_dir+\"/test_set\"+\"/test_noiseless_clones\"):\n",
    "        filename = os.fsdecode(file)\n",
    "        #print(re.sub(\"noiseless_\", \"\", filename, count=0, flags=0))\n",
    "        name_couples_test[filename]=\"noisy_\"+re.sub(\"noiseless_\", \"\", filename, count=0, flags=0)\n",
    "\n",
    "    name_couples_train = {}    \n",
    "    for file in os.listdir(main_dir+\"/train_set\"+\"/train_noiseless_clones\"):\n",
    "        filename = os.fsdecode(file)\n",
    "        #print(re.sub(\"noiseless_\", \"\", filename, count=0, flags=0))\n",
    "        name_couples_train[filename]=\"noisy_\"+re.sub(\"noiseless_\", \"\", filename, count=0, flags=0)\n",
    "\n",
    "    with open(main_dir+'/dataset_mapping_test.csv', mode='w') as map_test:\n",
    "\n",
    "        fieldnames = ['noiseless', 'noisy']\n",
    "        writer = csv.DictWriter(map_test, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "        for key in name_couples_test:\n",
    "            writer.writerow({fieldnames[0]:key, fieldnames[1]:name_couples_test[key]})\n",
    "\n",
    "    with open(main_dir+'/dataset_mapping_train.csv', mode='w') as map_test:\n",
    "\n",
    "        fieldnames = ['noiseless', 'noisy']\n",
    "        writer = csv.DictWriter(map_test, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "        for key in name_couples_train:\n",
    "            writer.writerow({fieldnames[0]:key, fieldnames[1]:name_couples_train[key]})\n",
    "\n",
    "outside_images_folder = \"/outside_data\"\n",
    "outside_directory_clean_clones = \"/outside_set\"\n",
    "\n",
    "\"\"\"\n",
    "Prepares images for usage through our model\n",
    "\n",
    "\"\"\"\n",
    "def getOutsideClones(outsideImg, sqr_size):\n",
    "    slices = array_divider(outsideImg,sqr_size)\n",
    "    for slice in slices:\n",
    "        yield slice\n",
    "\n",
    "\"\"\"\n",
    "Prepares images for usage through our model\n",
    "\n",
    "\"\"\"\n",
    "def prepareOutsideData_yield():\n",
    "\n",
    "    directory = os.fsencode(main_dir+outside_images_folder)\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        print(filename)\n",
    "\n",
    "        og_img=np.load(os.fsdecode(directory)+\"/\"+filename)\n",
    "\n",
    "    \n",
    "        clones = getOutsideClones(og_img,256)\n",
    "        index = 0\n",
    "        for clone in clones:\n",
    "            index+=1\n",
    "\n",
    "            #first member of the tuple is the rotate+flipper noise-FREE slice\n",
    "            PILimg = Image.fromarray(clone)\n",
    "            PILimg.convert(\"L\").save(main_dir+outside_directory_clean_clones+\"/outside_\"+filename[:len(filename) -4]+\"_\"+str(index)+\".png\")\n",
    "\n",
    "\"\"\"\n",
    "Creates a csv file with the file names list of \"outside\" pictures\n",
    "\n",
    "\"\"\"\n",
    "def create_outside_dataset_csv():\n",
    "    name_couples_outside = {}    \n",
    "    for file in os.listdir(main_dir+outside_directory_clean_clones):\n",
    "        filename = os.fsdecode(file)\n",
    "        #print(re.sub(\"noiseless_\", \"\", filename, count=0, flags=0))\n",
    "        name_couples_outside[filename]=filename\n",
    "\n",
    "    with open(main_dir+'/dataset_mapping_outside.csv', mode='w') as map_test:\n",
    "\n",
    "        fieldnames = ['noiseless', 'noisy']\n",
    "        writer = csv.DictWriter(map_test, fieldnames=fieldnames)\n",
    "\n",
    "        writer.writeheader()\n",
    "\n",
    "        for key in name_couples_outside:\n",
    "            writer.writerow({fieldnames[0]:key, fieldnames[1]:name_couples_outside[key]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ensure the directories are ready\n",
    "\n",
    "\"\"\"\n",
    "createIfNotExisting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpes1_21.npy\n",
      "height = 1076, width = 1076\n",
      "Damoh_21.npy\n",
      "height = 1026, width = 1026\n",
      "DesMoines_21.npy\n",
      "height = 2143, width = 2795\n",
      "Rennes_26.npy\n",
      "height = 1931, width = 1159\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creates sliced clones (256x256 by default) of original images and puts them in the \"./data2/train_set/  train_noiseless_clones and train_noisy_clones\" folders\n",
    "\n",
    "\"\"\"\n",
    "createData_yield()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Moves a fraction of the train set to the test set (default=10%)\n",
    "\n",
    "\"\"\"\n",
    "sort_data_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a csv file with the file names list of \"train\" and \"test\" pictures for easier dataset loading once in torch's models\n",
    "\n",
    "\"\"\"\n",
    "create_dataset_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lely.npy\n",
      "height = 500, width = 500\n",
      "marais1.npy\n",
      "height = 500, width = 500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prepares images for usage through our model\n",
    "\n",
    "\"\"\"\n",
    "prepareOutsideData_yield()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a csv file with the file names list of \"outside\" pictures\n",
    "\n",
    "\"\"\"\n",
    "create_outside_dataset_csv()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d77f5c09424df69426905ef239432a9664f700bba570fd823360d31369285ad8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('LearningImage-Ys9ocoUw': pipenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
